{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "collapsed_sections": ["kxrUseR_X-ci"]}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["# **\u00dcbung 4a** Programmierung mit Python mit Anwendungen aus dem Maschinellen Lernen"], "metadata": {"id": "tO4WGLA7VZuQ"}}, {"cell_type": "markdown", "source": ["## Aufgabe 1\n", "\n", "Beim Maschinellen Lernen k\u00f6nnen Klassen helfen, verschiedene Implementierungen von Algorithmen auf den wesentlichen Schnittstellen zu abstrahieren. Um sich selbst ein Bild davon zu machen, schreiben Sie einen kleinen Evaluator, der neben der `LinearRegression` auch noch `ElasticNet` und `SGDRegressor` (oder andere) wiederholt auf verschiedenen Eingangdaten testet. F\u00fchren Sie dazu eine Regression wiederholt mit jeder der drei Klassen und verschiedenen Eingangsdaten durch. Variiren Sie die Koeffizienten `c0`, `c1` und `noise_factor` \u00fcbergeben. Ermitteln Sie jeweils den Fehler zu den vorgegebenen Koeffizienten `c0` und `c1`. Bei `c0` und `c1` handelt es sich um Koeffizienten einer Gerade (Steigung und Achsenabschnitt).\n", "\n", "Nachfolgende Zelle enth\u00e4lt die Funktion `get_linear_relationship_sample`. Diese generiert weiterhin, wie in \u00dcbung 4, Werte nach folgender linearen Gleichung $f(x) = c0 + x*c1 + N(x)$. $N(x)$ stellt ein von $x$ abh\u00e4ngiges Rauschen da."], "metadata": {"id": "kxrUseR_X-ci"}}, {"cell_type": "code", "source": ["import random\n", "import numpy as np\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LinearRegression, ElasticNet, SGDRegressor\n", "\n", "\n", "def get_value(x, c0, c1, nf):\n", "    return c0 + c1 * x + (random.random()-0.5) * x * x * nf\n", "\n", "\n", "def get_linear_relationship_sample(c0=9, c1=4, noise_factor=0.1):\n", "    '''\n", "    Erzeugt Punkte in einem bestimmten Bereich nach folgender Funktion f(x) = c0 + x*c1 + N(x)\n", "    N ist ein Zufallsterm\n", "    '''\n", "    X, y = zip(*[(x, get_value(x, c0=c0, c1=c1, nf=noise_factor))\n", "                 for x in np.arange(0.1, 15, 0.01)])\n", "    X = np.array(X).reshape((len(X), 1))\n", "    return train_test_split(\n", "        X, y, test_size=0.2, random_state=0)\n"], "metadata": {"id": "ZAgD0W8WJzHG"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["p_n_loop = 100\n", "noise_factors = [0.1, 2]\n", "c0s = [0, 40]\n", "c1s = [-1, 4]\n", "regressors = [LinearRegression, ElasticNet, SGDRegressor]\n", "\n", "\n", "def get_first_value_or_value(a):\n", "    if isinstance(a, list):\n", "        if a:\n", "            return a[0]\n", "        else:\n", "            raise AssertionError('Should never happen')\n", "    else:\n", "        return a\n", "\n", "\n", "def test_regressor(n_loop, c0, c1, noise_factor, regressor):\n", "    result_c0s = []\n", "    result_c1s = []\n", "    print(f'Testing: {n_loop}, {c0}, {c1}, {noise_factor}, {regressor}')\n", "    for _ in range(n_loop):\n", "        X_train, _, y_train, _ = get_linear_relationship_sample(\n", "            c0=c0, c1=c1, noise_factor=noise_factor)\n", "        r = regressor()\n", "        r.fit(X_train, y_train)\n", "        result_c0s.append(get_first_value_or_value(r.intercept_))\n", "        result_c1s.append(get_first_value_or_value(r.coef_))\n", "    print(\n", "        f'Result: c0 Error {np.square(np.array(result_c0s) - c0).mean()}')\n", "    print(\n", "        f'Result: c1 Error {np.square(np.array(result_c1s) - c1).mean()}')\n", "    print()\n", "\n", "\n", "for p_noise_factor in noise_factors:\n", "    print(f'Set noise_factor to {p_noise_factor}')\n", "    for p_c0 in c0s:\n", "        for p_c1 in c1s:\n", "            for p_regressor in regressors:\n", "                test_regressor(p_n_loop, p_c0, p_c1,\n", "                               p_noise_factor, p_regressor)\n"], "metadata": {"id": "_x0Fp2l2Cn1a"}, "execution_count": null, "outputs": []}]}